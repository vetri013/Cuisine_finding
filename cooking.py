# -*- coding: utf-8 -*-
"""Cooking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eb2avVOMt-9BYXnSy9ZVBNX6g7b1CQQB
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_json('/content/drive/MyDrive/dataset/DL Assignment/train.json')

df.head()

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
df.cuisine.value_counts().plot.bar(title='Classes Counts')
plt.show()

from sklearn.preprocessing import LabelEncoder

n_classes = len(df['cuisine'].unique())
print("Number of classes", n_classes)

# get the length of the tokens
df['length'] = df.ingredients.map(lambda x: len(x))

# get the number of classes
le = LabelEncoder()
df['categorical_label'] = le.fit_transform(df.cuisine)
df.head()


df['cuisine'].unique()

df[df["cuisine"]=='russian']

from sklearn.model_selection import train_test_split

# split dataset
train_set, valid_set = train_test_split(df, test_size=0.15, stratify=df.cuisine, random_state=42)

print(train_set.shape)
print(valid_set.shape)

train_sentences = [','.join(sentence) for sentence in train_set.ingredients.values.tolist()]
valid_sentences = [','.join(sentence) for sentence in valid_set.ingredients.values.tolist()]

# get the labels
y_train = train_set.categorical_label
y_valid = valid_set.categorical_label

train_sentences[:3]

train_sentences

import tensorflow as tf

# get sequence max length
sequence_length = int(df['length'].max())

# create vectorization layer
vectorization_layer = tf.keras.layers.TextVectorization(max_tokens=None, output_mode='int', output_sequence_length=sequence_length,
                                                        split=lambda x: tf.strings.split(x, ','), standardize=lambda x: tf.strings.lower(x))
vectorization_layer.adapt(train_sentences)

# create vectorization layer
vectorizer = tf.keras.models.Sequential()
vectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))
vectorizer.add(vectorization_layer)

# get sequences
train_sequences = vectorizer.predict(train_sentences)
valid_sequences = vectorizer.predict(valid_sentences)

print(train_sentences[:3])
print(train_sequences[:3])

sequence_length

print(len(vectorization_layer.get_vocabulary()))m
print(vectorization_layer.get_vocabulary()[:10])

embedding_dim = 50
vocab_size = vectorization_layer.vocabulary_size()

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length, mask_zero=True),
    tf.keras.layers.Conv1D(128,5, activation='relu'),
    tf.keras.layers.GlobalMaxPool1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(n_classes, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("cooking_deep.h5", save_best_only=True)

history = model.fit(train_sequences, y_train, epochs=30, validation_data=(valid_sequences, y_valid),
                    callbacks=[early_stopping_cb, checkpoint_cb])

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = 30  # Get number of epochs

# Plot training and validation loss per epoch
plt.figure(figsize=(8, 6))
plt.plot(epochs, loss, 'r', label="Training Loss")
plt.plot(epochs, val_loss, 'b', label="Validation Loss")
plt.legend()
plt.show()

model = tf.keras.models.load_model("cooking_deep.h5")
print(model.evaluate(valid_sequences, y_valid))

y_test1=pd.read_json("/content/drive/MyDrive/dataset/DL Assignment/test.json")

test_sentence = [','.join(sentence) for sentence in y_test1.ingredients.values.tolist()]
y_test=vectorizer.predict(test_sentence)

def predict(n1,n2,n3,n4):

   t=[n1,n2,n3,n4]
   n=[",".join(sent for sent in t)]
   new=vectorizer.predict(n)
   predictions=model.predict(new)
   res=le.inverse_transform(np.argmax(predictions, axis=1))
   return res

predict(["milk","bread","sausage","hot"]'')





y_test1["cuisine"] = le.inverse_transform(np.argmax(predictions, axis=1))

y_test1["cuisine"]

import pandas as pd
import numpy as np
import pickle
import streamlit as st
from PIL import Image




# defining the function which will make the prediction using
# the data which the user inputs
def prediction(indgred1,indgred2,indgred3,indgred4):
    ingred=list[indgred1,indgred2,indgred3,indgred4]
    model = tf.keras.models.load_model("cooking_deep.h5")
    indgred_sentence=[','.join(indgred)]
    indgred_test=vectorizer.predict(indgred_sentence)
    new_predictions=model.predict(y_test)
    ingred_cuisine= le.inverse_transform(np.argmax(predictions, axis=1))
    return ingred_cuisine


# this is the main function in which we define our webpage
def main():
      # giving the webpage a title
    st.title("Cuisine Classification")

    # here we define some of the front end elements of the web page like
    # the font and background color, the padding and the text to be displayed
    html_temp = """
    <div style ="background-color:yellow;padding:13px">
    <h1 style ="color:black;text-align:center;">Streamlit Iris Flower Classifier ML App </h1>
    </div>
    """

    # this line allows us to display the front end aspects we have
    # defined in the above code
    st.markdown(html_temp, unsafe_allow_html = True)

    # the following lines create text boxes in which the user can enter
    # the data required to make the prediction
    indgred1= st.text_input("indgedients1")
    indgred2= st.text_input("indgedients2")
    indgred3= st.text_input("indgedients3")
    indgred4= st.text_input("indgedients4")
    result=''
    # the below line ensures that when the button called 'Predict' is clicked,
    # the prediction function defined above is called to make the prediction
    # and store it in the variable result
    if st.button("Predict"):
        result = prediction(indgred1,indgred2,indgred3,indgred4)
    st.success('The Cuisine is {}'.format(result))

if __name__=='__main__':
    main()

!pip install streamlit  ["sausage links",
      "fennel bulb",
      "fronds",
      "olive oil",
      "cuban peppers",
      "onions"]

!streamlit run app.py

!pip install tensorflow

